{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9b2506d",
   "metadata": {},
   "source": [
    "# BioMed: Information Retrieval - BioMedical Information Retrieval System\n",
    "\n",
    "---\n",
    "\n",
    "**Group:**\n",
    "- Reyes Castro, Didier Yamil (didier.reyes.castro@alumnos.upm.es)\n",
    "- Rodriguez Fernández, Cristina ()\n",
    "\n",
    "**Course:** BioMedical Informatics - 2025/26\n",
    "\n",
    "**Institution:** Polytechnic University of Madrid (UPM)\n",
    "\n",
    "**Date:** November 2026\n",
    "\n",
    "---\n",
    "\n",
    "## Goal\n",
    "\n",
    "To develop an Information Retrieval system — specifically, a **binary text classifier** — to identify scientific articles in the PubMed database that are related to a given set of abstracts within a defined research topic. In this case, the focus is on a collection of 1,308 manuscripts containing information on the polyphenol composition of various foods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce86cc1",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e05604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn pandas requests transformers pytorch datasets numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa50003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6641b8",
   "metadata": {},
   "source": [
    "## **Task 1:** \n",
    "\n",
    "Retrieve from PubMed the abstracts associated with each publication in publications.xlsx\n",
    "\n",
    "(21 minutes with API KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e505653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n",
    "ESEARCH_URL = BASE_URL + \"esearch.fcgi\"\n",
    "FETCH_URL = BASE_URL + \"efetch.fcgi\"\n",
    "DS_WITH_PMID = 'publications_pmid.csv'\n",
    "PMID_ABSTRACTS = 'publications_pmid_abstract.csv'\n",
    "ELSEVIER_SEARCH_URL = \"https://api.elsevier.com/content/search/scopus\"\n",
    "ELSEVIER_ABSTRACTS = 'publications_abstract_pubmed_elsevier.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413867ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('publications.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7c3c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Search for the PMID of the articles by title\n",
    "def search_pmid(article):\n",
    "    \n",
    "    title = article['title']\n",
    "    params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"term\": f\"{title}\",\n",
    "        \"retmode\": \"json\",\n",
    "        \"field\": \"title\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Trying to find the PMID\n",
    "        response = requests.get(ESEARCH_URL, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        if len(data[\"esearchresult\"][\"idlist\"]) >= 1:\n",
    "            pmid = data['esearchresult']['idlist'][0]\n",
    "            print(f\"> Found PMID for article: {pmid}\")\n",
    "            return pmid\n",
    "        \n",
    "        print(f\"> No PMID found for article.\")\n",
    "        return None\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"> ERROR during request for article: {e}\")\n",
    "        return None\n",
    "    \n",
    "ds_pmid = dataset.copy()\n",
    "for idx, article in ds_pmid.iterrows():\n",
    "    print(f\"[{idx + 1}/{len(ds_pmid)}] Searching PMID for: {article['title']}\")\n",
    "    pmid = search_pmid(article)\n",
    "    ds_pmid.at[idx, 'pmid'] = pmid\n",
    "\n",
    "ds_pmid.to_csv(DS_WITH_PMID, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4cf173",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of articles with PMID found:\", ds_pmid['pmid'].notnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447a6a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Fetch article abstract by PMID\n",
    "def fetch_abstract_by_pmid(pmid):\n",
    "    params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"id\": f\"{pmid}\",\n",
    "        \"retmode\": \"text\",\n",
    "        \"rettype\": \"abstract\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(FETCH_URL, params=params)\n",
    "        response.raise_for_status()\n",
    "        print(f\"> Fetched abstract!!\")\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"> ERROR fetching abstract for PMID '{pmid}': {e}\")\n",
    "        return None\n",
    "    \n",
    "ds_pmid_abstract = ds_pmid.copy()\n",
    "for idx, article in ds_pmid_abstract.iterrows():\n",
    "    pmid = article['pmid']\n",
    "    if pd.notnull(pmid):\n",
    "        print(f\"[{idx + 1}/{len(ds_pmid_abstract)}] Fetching abstract for PMID: {pmid}\")\n",
    "        abstract = fetch_abstract_by_pmid(pmid)\n",
    "        ds_pmid_abstract.at[idx, 'abstract'] = abstract\n",
    "    else:\n",
    "        print(f\"[{idx + 1}/{len(ds_pmid_abstract)}] No PMID available, skipping abstract fetch.\")\n",
    "        ds_pmid_abstract.at[idx, 'abstract'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6b2b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pmid_abstract.to_csv(PMID_ABSTRACTS, index=False)\n",
    "print(\"Number of articles with abstract fetched:\", ds_pmid_abstract['abstract'].notnull().sum())\n",
    "print(\"Number of articles without abstract fetched:\", ds_pmid_abstract['abstract'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cfa3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_scopus(article_row, api_key):\n",
    "\n",
    "    try:\n",
    "\n",
    "        title = article_row.get('title', '').replace('\"', '') # Remove quotes for query\n",
    "        headers = {\"Accept\": \"application/json\"}\n",
    "        params = {\n",
    "            \"query\": f\"TITLE-ABS-KEY(\\\"{title}\\\")\",\n",
    "            \"apiKey\": api_key\n",
    "        }\n",
    "        \n",
    "        response = requests.get(ELSEVIER_SEARCH_URL, \n",
    "                                headers=headers, params=params, timeout=10)\n",
    "        response.raise_for_status() \n",
    "        data = response.json()\n",
    "\n",
    "        if 'search-results' in data and data['search-results']['entry']:\n",
    "            try:\n",
    "                return data['search-results']['entry'][0]['prism:url']\n",
    "            except KeyError:\n",
    "                return None\n",
    "        return None\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"> Scopus ERROR: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65686071",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_elsevier = pd.read_csv(PMID_ABSTRACTS)\n",
    "\n",
    "counters = {\n",
    "    'total_missing': ds_elsevier['abstract'].isnull().sum(),\n",
    "    'elsevier_found': 0,\n",
    "    'failed': 0\n",
    "}\n",
    "\n",
    "for i, row in ds_elsevier.iterrows():\n",
    "\n",
    "    if not pd.isnull(row['abstract']):\n",
    "        continue\n",
    "\n",
    "    print(f\"[{i + 1}/{len(ds_elsevier)}] Searching ELSEVIER for abstract of article: {row['title']}\")\n",
    "    abstract_url = search_scopus(row, \"3f5ff36eb8d1d409e3befea2ed2aa2cc\")\n",
    "    \n",
    "    if abstract_url:\n",
    "        response = requests.get(abstract_url, \n",
    "                                headers={\"Accept\": \"application/json\", \n",
    "                                         \"X-ELS-APIKey\": \"3f5ff36eb8d1d409e3befea2ed2aa2cc\"})\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            try:\n",
    "                abstract_text = data['abstracts-retrieval-response']['coredata']['dc:description']\n",
    "                ds_elsevier.at[i, 'abstract'] = abstract_text\n",
    "                print(\"> Found abstract via ELSEVIER!\")\n",
    "                counters['elsevier_found'] += 1\n",
    "            except KeyError:\n",
    "                print(\"> Abstract not found in ELSEVIER response.\")\n",
    "                counters['failed'] += 1\n",
    "    else:\n",
    "        print(\"> Nope :(\")\n",
    "        counters['failed'] += 1\n",
    "\n",
    "    time.sleep(1) # Polite 1-second delay\n",
    "\n",
    "ds_elsevier.to_csv(ELSEVIER_ABSTRACTS, index=False)\n",
    "print(\"Summary of ELSEVIER abstract search:\")\n",
    "print(f\"Total missing abstracts at start: {counters['total_missing']}\")\n",
    "print(f\"Abstracts found via ELSEVIER: {counters['elsevier_found']}\")\n",
    "print(f\"Failed attempts: {counters['failed']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207ecedb",
   "metadata": {},
   "source": [
    "TODO: ENCONTRAR LOS ~100 ARTÍCULOS QUE FALTAN @CRISTINA. SI NO A MANO. EL ULTIMO CSV ES \"publications_abstract_pubmed_elsevier.csv\", PARTIR DE AHÍ.\n",
    "\n",
    "LUEGO EL RESTO DE CODIGO DEBERÍA FUNCIONAR BIEN AUNQUE HAY QUE CAMBIAR EL DATASET FINAL Y ALGUNAS VARIABLES. SINO PREGUNTAR A @DIDIER.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b780b",
   "metadata": {},
   "source": [
    "## **Task 2:**\n",
    "\n",
    "Use the EUtilities tool to search for articles whose content is not relevant to this task. Size of the dataset should be the same of relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c625aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles_pmids_for_title(title, count, api_key=None):\n",
    "    \n",
    "    params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"term\": f\"{title}[Title]\",\n",
    "        \"retmode\": \"json\",\n",
    "        \"retmax\": count,\n",
    "        \"api_key\": api_key\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(ESEARCH_URL, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        if 'esearchresult' in data and data['esearchresult']['count'] != '0':\n",
    "            return data['esearchresult']['idlist']\n",
    "        else:\n",
    "            print(f\"Found {data['esearchresult']['count']} irrelevant articles.\")\n",
    "            return []\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error during request for irrelevant articles: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a0f689",
   "metadata": {},
   "outputs": [],
   "source": [
    "IRRELEVANT_PUBLICATIONS = 'irrelevant_publications.csv'\n",
    "\n",
    "final_df = pd.read_csv('publications_abstract_pubmed_elsevier.csv')\n",
    "\n",
    "irrelevant_pmids_list = get_articles_pmids_for_title(\"cancer\", len(final_df))\n",
    "\n",
    "irrelevant_abstracts = []\n",
    "for pmid in irrelevant_pmids_list:\n",
    "\n",
    "    article_info = {\n",
    "        'pmid': pmid,\n",
    "        'abstract': None\n",
    "    }\n",
    "\n",
    "    article_info['abstract'] = fetch_abstract_by_pmid(pmid)\n",
    "    irrelevant_abstracts.append(article_info)\n",
    "\n",
    "    # CHANGE ME TO 0.1 IF YOU HAVE AN API KEY\n",
    "    print(\"Sleeping for 0.1...\")\n",
    "    time.sleep(0.1)  # Delaying 0.1s to respect NCBI rate limits (3 requests per second)\n",
    "\n",
    "# Save irrelevant abstracts to a new dataset\n",
    "irrelevant_df = pd.DataFrame(irrelevant_abstracts)\n",
    "irrelevant_df.to_csv(IRRELEVANT_PUBLICATIONS, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904e0778",
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevant_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5092b356",
   "metadata": {},
   "source": [
    "## **Task 4:**\n",
    "\n",
    "Implement the chosen retrieval system using the programming language of their choice. If the information retrieval system is based on machine learning techniques, the student must split the existing datasets (relevant and non-relevant documents) into three distinct groups (training, validation, and testing) to carry out the model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec80f4b4",
   "metadata": {},
   "source": [
    "**CHOSEN RETRIEVAL SYSTEM:** BioBERT-based Binary Text Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8708cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding target variable 'relevance' \n",
    "relevant_df['relevance'] = 1\n",
    "irrelevant_df['relevance'] = 0\n",
    "\n",
    "# Combining relevant and irrelevant datasets and maintaining only abstract and relevance columns\n",
    "features = ['abstract', 'relevance']\n",
    "combined_df = pd.concat([relevant_df[features], irrelevant_df[features]], ignore_index=True)\n",
    "\n",
    "# Remove any rows where the abstract is missing (e.g., API fetch failed)\n",
    "combined_df.dropna(subset=['abstract'], inplace=True)\n",
    "combined_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Saving\n",
    "combined_df.to_csv('combined_publications.csv', index=False)\n",
    "\n",
    "print(\"Class distribution:\")\n",
    "print(combined_df['relevance'].value_counts())\n",
    "\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec449e83",
   "metadata": {},
   "source": [
    "Following Fine-tuning of BERT for text classification tasks: https://huggingface.co/docs/transformers/en/tasks/sequence_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b8bad6",
   "metadata": {},
   "source": [
    "- Train-Test-Validation Split: 80%-10%-10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b988a960",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "\n",
    "train_df, test_df = train_test_split(combined_df,\n",
    "                                     test_size=0.2,\n",
    "                                     stratify=combined_df[\"relevance\"],\n",
    "                                     random_state=RANDOM_STATE)\n",
    "\n",
    "val_df, test_df = train_test_split(test_df,\n",
    "                                   test_size=0.5,\n",
    "                                   stratify=test_df[\"relevance\"],\n",
    "                                   random_state=RANDOM_STATE)\n",
    "\n",
    "print(f\"Training size: {len(train_df)}\")\n",
    "print(f\"Validation size: {len(val_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b0e10b",
   "metadata": {},
   "source": [
    "- Convert Pandas DataFrame to HuggingFace Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67eea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e430ee07",
   "metadata": {},
   "source": [
    "- Tokenization of abstracts using BioBERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf38265",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_NAME = \"dmis-lab/biobert-v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"abstract\"], \n",
    "                     padding=\"max_length\", \n",
    "                     truncation=True,\n",
    "                     max_length=512 # Maximum length for BERT models\n",
    "                    )\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Renaming the target column to 'labels' as expected by HuggingFace Trainer\n",
    "train_dataset = train_dataset.rename_column(\"relevance\", \"labels\")\n",
    "val_dataset = val_dataset.rename_column(\"relevance\", \"labels\")\n",
    "test_dataset = test_dataset.rename_column(\"relevance\", \"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c95d8c",
   "metadata": {},
   "source": [
    "- Loading BioBERT model for binary text classification (relevant vs irrelevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a8a12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"irrelevant\", 1: \"relevant\"}\n",
    "label2id = {\"irrelevant\": 0, \"relevant\": 1}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(BERT_MODEL_NAME, \n",
    "                                                           num_labels=2,\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554adc57",
   "metadata": {},
   "source": [
    "- Defining evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb597090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, \n",
    "                                                               predictions, \n",
    "                                                               average=\"binary\",\n",
    "                                                               zero_division=0)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b701845f",
   "metadata": {},
   "source": [
    "- Putting the training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e7d046",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./biobert_pubmed_classifier\",\n",
    "\n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "\n",
    "    # Optimiser settings\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # Evaluation settings\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "\n",
    "    # Model selection    \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "\n",
    "    # Performance\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=4,\n",
    "\n",
    "    seed=RANDOM_STATE,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c4d0a6",
   "metadata": {},
   "source": [
    "- Actual training using Trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744c7adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7795ee5",
   "metadata": {},
   "source": [
    "- Evaluating on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ba41f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_output = trainer.predict(test_dataset)\n",
    "predictions = np.argmax(predictions_output.predictions, axis=-1)\n",
    "true_labels = predictions_output.label_ids\n",
    "\n",
    "# Calculate all metrics\n",
    "test_metrics = compute_metrics((predictions_output.predictions, true_labels))\n",
    "\n",
    "print(\"\\nTest Set Results:\")\n",
    "print(f\"Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"Recall:    {test_metrics['recall']:.4f}\")\n",
    "print(f\"F1-Score:  {test_metrics['f1']:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    true_labels, \n",
    "    predictions,\n",
    "    target_names=['Irrelevant', 'Relevant'],\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "print(cm)\n",
    "print(f\"\\nTrue Negatives:  {cm[0][0]} (correctly identified irrelevant)\")\n",
    "print(f\"False Positives: {cm[0][1]} (incorrectly marked relevant)\")\n",
    "print(f\"False Negatives: {cm[1][0]} (missed relevant papers)\")\n",
    "print(f\"True Positives:  {cm[1][1]} (correctly identified relevant)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50be2fb",
   "metadata": {},
   "source": [
    "- saving the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4858b6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = './final_biobert_classifier'\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomed_ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
