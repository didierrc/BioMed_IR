{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9b2506d",
   "metadata": {},
   "source": [
    "# BioMed: Information Retrieval - BioMedical Information Retrieval System\n",
    "\n",
    "---\n",
    "\n",
    "**Group:**\n",
    "- Reyes Castro, Didier Yamil (didier.reyes.castro@alumnos.upm.es)\n",
    "- Rodriguez Fernández, Cristina ()\n",
    "\n",
    "**Course:** BioMedical Informatics - 2025/26\n",
    "\n",
    "**Institution:** Polytechnic University of Madrid (UPM)\n",
    "\n",
    "**Date:** November 2026\n",
    "\n",
    "---\n",
    "\n",
    "## Goal\n",
    "\n",
    "To develop an Information Retrieval system — specifically, a **binary text classifier** — to identify scientific articles in the PubMed database that are related to a given set of abstracts within a defined research topic. In this case, the focus is on a collection of 1,308 manuscripts containing information on the polyphenol composition of various foods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce86cc1",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e05604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn pandas requests transformers pytorch datasets numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aa50003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6641b8",
   "metadata": {},
   "source": [
    "## **Task 1:** \n",
    "\n",
    "Retrieve from PubMed the abstracts associated with each publication in publications.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413867ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('publications.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7c3c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n",
    "ESEARCH_URL = BASE_URL + \"esearch.fcgi\"\n",
    "FETCH_URL = BASE_URL + \"efetch.fcgi\"\n",
    "\n",
    "# Step 1: Search for the PMID of the article by title\n",
    "def search_pmid_by_title(title, api_key=None):\n",
    "    params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"term\": f\"{title}[Title]\",\n",
    "        \"retmode\": \"json\",\n",
    "        \"api_key\": api_key\n",
    "    }\n",
    "\n",
    "    try:\n",
    "\n",
    "        response = requests.get(ESEARCH_URL, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        if 'esearchresult' in data and data['esearchresult']['count'] != '0':\n",
    "            return data['esearchresult']['idlist'][0]\n",
    "        else:\n",
    "            print(f\"Found {data['esearchresult']['count']} PMIDs for title: {title}. Skipping...\")\n",
    "            return None\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error during request for title '{title}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Step 2: Fetch article abstract by PMID\n",
    "def fetch_abstract_by_pmid(pmid, api_key=None):\n",
    "    params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"id\": pmid,\n",
    "        \"retmode\": \"text\",\n",
    "        \"rettype\": \"abstract\",\n",
    "        \"api_key\": api_key\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(FETCH_URL, params=params)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching abstract for PMID '{pmid}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Process each article in the dataset\n",
    "relevant_abstracts = []\n",
    "for i, article in dataset.iterrows():\n",
    "\n",
    "    article_info = {\n",
    "        'id': article['id'],\n",
    "        'pmid': None,\n",
    "        'title': article['title'],\n",
    "        'abstract': None\n",
    "    }\n",
    "\n",
    "    title = article['title']\n",
    "    pmid = search_pmid_by_title(title)\n",
    "    \n",
    "    if pmid:\n",
    "        article_info['pmid'] = pmid\n",
    "        abstract = fetch_abstract_by_pmid(pmid)\n",
    "        article_info['abstract'] = abstract\n",
    "\n",
    "    relevant_abstracts.append(article_info)\n",
    "\n",
    "    # CHANGE ME TO 0.1 IF YOU HAVE AN API KEY\n",
    "    print(\"Sleeping for 1...\")\n",
    "    time.sleep(1)  # Delaying 1s to respect NCBI rate limits (3 requests per second)\n",
    "\n",
    "# Add relevant_abstracts to a new dataset\n",
    "relevant_df = pd.DataFrame(relevant_abstracts)\n",
    "\n",
    "# Save the updated dataset\n",
    "relevant_df.to_csv('publications_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4944556",
   "metadata": {},
   "source": [
    "There are a lot of PMIDs whose abstract is not available :( ... Ask professor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b780b",
   "metadata": {},
   "source": [
    "## **Task 2:**\n",
    "\n",
    "Use the EUtilities tool to search for articles whose content is not relevant to this task. Size of the dataset should be the same of relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c625aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_irrelevant_articles(term, count, api_key=None):\n",
    "    \n",
    "    print(f\"Fetching {count} irrelevant articles...\")\n",
    "\n",
    "    params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"term\": term,\n",
    "        \"retmode\": \"json\",\n",
    "        \"retmax\": count,\n",
    "        \"api_key\": api_key\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(ESEARCH_URL, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        if 'esearchresult' in data and data['esearchresult']['count'] != '0':\n",
    "            return data['esearchresult']['idlist']\n",
    "        else:\n",
    "            print(f\"Found {data['esearchresult']['count']} irrelevant articles.\")\n",
    "            return []\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error during request for irrelevant articles: {e}\")\n",
    "        return []\n",
    "\n",
    "irrelevant_pmids_list = search_irrelevant_articles(\"cancer[Title]\", len(dataset))\n",
    "\n",
    "irrelevant_abstracts = []\n",
    "for pmid in irrelevant_pmids_list:\n",
    "\n",
    "    article_info = {\n",
    "        'pmid': pmid,\n",
    "        'abstract': None\n",
    "    }\n",
    "\n",
    "    article_info['abstract'] = fetch_abstract_by_pmid(pmid)\n",
    "    irrelevant_abstracts.append(article_info)\n",
    "\n",
    "    # CHANGE ME TO 0.1 IF YOU HAVE AN API KEY\n",
    "    print(\"Sleeping for 1...\")\n",
    "    time.sleep(1)  # Delaying 1s to respect NCBI rate limits (3 requests per second)\n",
    "\n",
    "# Save irrelevant abstracts to a new dataset\n",
    "irrelevant_df = pd.DataFrame(irrelevant_abstracts)\n",
    "irrelevant_df.to_csv('irrelevant_publications.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6d352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: CHECK THIS OUT!! SEEMS NOT TO BE WORKING\n",
    "# There are strange abstract in the irrelevant dataset like \"1.\", erasing them and researching...\n",
    "# irrelevant_dataset_cleaned = irrelevant_dataset[~irrelevant_dataset['abstract'].str.match('1.')]\n",
    "#\n",
    "# irrelevant_dataset_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a372c85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching other irrelevant abstracts\n",
    "# new_irrelevant_pmids_list = search_irrelevant_articles(\"pneumonia[Title]\", len(dataset) - len(irrelevant_dataset), \"8e029cc2ba291ed9ee30e494f27c18017408\")\n",
    "# new_irrelevant_abstracts = []\n",
    "# for pmid in new_irrelevant_pmids_list:\n",
    "#     abstract = fetch_abstract_by_pmid(pmid, \"8e029cc2ba291ed9ee30e494f27c18017408\")\n",
    "#     new_irrelevant_abstracts.append(abstract)\n",
    "#     print(\"Sleeping for 0.1...\")\n",
    "#     time.sleep(0.1)  # Delaying 0.1s to respect NCBI rate limits (10 requests per second)\n",
    "# \n",
    "# # Adding abstracts to the irrelevant dataset\n",
    "# new_irrelevant_dataset = pd.DataFrame({'pmid': new_irrelevant_pmids_list, 'abstract': new_irrelevant_abstracts})\n",
    "# irrelevant_dataset = pd.concat([irrelevant_dataset, new_irrelevant_dataset], ignore_index=True)\n",
    "# irrelevant_dataset.to_csv('irrelevant_publications_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5092b356",
   "metadata": {},
   "source": [
    "## **Task 4:**\n",
    "\n",
    "Implement the chosen retrieval system using the programming language of their choice. If the information retrieval system is based on machine learning techniques, the student must split the existing datasets (relevant and non-relevant documents) into three distinct groups (training, validation, and testing) to carry out the model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec80f4b4",
   "metadata": {},
   "source": [
    "**CHOSEN RETRIEVAL SYSTEM:** BioBERT-based Binary Text Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8708cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding target variable 'relevance' \n",
    "relevant_df['relevance'] = 1\n",
    "irrelevant_df['relevance'] = 0\n",
    "\n",
    "# Combining relevant and irrelevant datasets and maintaining only abstract and relevance columns\n",
    "features = ['abstract', 'relevance']\n",
    "combined_df = pd.concat([relevant_df[features], irrelevant_df[features]], ignore_index=True)\n",
    "\n",
    "# Remove any rows where the abstract is missing (e.g., API fetch failed)\n",
    "combined_df.dropna(subset=['abstract'], inplace=True)\n",
    "combined_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Saving\n",
    "combined_df.to_csv('combined_publications.csv', index=False)\n",
    "\n",
    "print(\"Class distribution:\")\n",
    "print(combined_df['relevance'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec449e83",
   "metadata": {},
   "source": [
    "Following Fine-tuning of BERT for text classification tasks: https://huggingface.co/docs/transformers/en/tasks/sequence_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b8bad6",
   "metadata": {},
   "source": [
    "- Train-Test-Validation Split: 80%-10%-10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b988a960",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "\n",
    "train_df, test_df = train_test_split(combined_df,\n",
    "                                     test_size=0.2,\n",
    "                                     stratify=combined_df[\"relevance\"],\n",
    "                                     random_state=RANDOM_STATE)\n",
    "\n",
    "val_df, test_df = train_test_split(test_df,\n",
    "                                   test_size=0.5,\n",
    "                                   stratify=test_df[\"relevance\"],\n",
    "                                   random_state=RANDOM_STATE)\n",
    "\n",
    "print(f\"Training size: {len(train_df)}\")\n",
    "print(f\"Validation size: {len(val_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b0e10b",
   "metadata": {},
   "source": [
    "- Convert Pandas DataFrame to HuggingFace Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67eea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e430ee07",
   "metadata": {},
   "source": [
    "- Tokenization of abstracts using BioBERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf38265",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_NAME = \"dmis-lab/biobert-v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"abstract\"], \n",
    "                     padding=\"max_length\", \n",
    "                     truncation=True,\n",
    "                     max_length=512 # Maximum length for BERT models\n",
    "                    )\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Renaming the target column to 'labels' as expected by HuggingFace Trainer\n",
    "train_dataset = train_dataset.rename_column(\"relevance\", \"labels\")\n",
    "val_dataset = val_dataset.rename_column(\"relevance\", \"labels\")\n",
    "test_dataset = test_dataset.rename_column(\"relevance\", \"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c95d8c",
   "metadata": {},
   "source": [
    "- Loading BioBERT model for binary text classification (relevant vs irrelevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a8a12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"irrelevant\", 1: \"relevant\"}\n",
    "label2id = {\"irrelevant\": 0, \"relevant\": 1}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(BERT_MODEL_NAME, \n",
    "                                                           num_labels=2,\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554adc57",
   "metadata": {},
   "source": [
    "- Defining evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb597090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, \n",
    "                                                               predictions, \n",
    "                                                               average=\"binary\",\n",
    "                                                               zero_division=0)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b701845f",
   "metadata": {},
   "source": [
    "- Putting the training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e7d046",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./biobert_pubmed_classifier\",\n",
    "\n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "\n",
    "    # Optimiser settings\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # Evaluation settings\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "\n",
    "    # Model selection    \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "\n",
    "    # Performance\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=4,\n",
    "\n",
    "    seed=RANDOM_STATE,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c4d0a6",
   "metadata": {},
   "source": [
    "- Actual training using Trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744c7adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7795ee5",
   "metadata": {},
   "source": [
    "- Evaluating on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ba41f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_output = trainer.predict(test_dataset)\n",
    "predictions = np.argmax(predictions_output.predictions, axis=-1)\n",
    "true_labels = predictions_output.label_ids\n",
    "\n",
    "# Calculate all metrics\n",
    "test_metrics = compute_metrics((predictions_output.predictions, true_labels))\n",
    "\n",
    "print(\"\\nTest Set Results:\")\n",
    "print(f\"Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"Recall:    {test_metrics['recall']:.4f}\")\n",
    "print(f\"F1-Score:  {test_metrics['f1']:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    true_labels, \n",
    "    predictions,\n",
    "    target_names=['Irrelevant', 'Relevant'],\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "print(cm)\n",
    "print(f\"\\nTrue Negatives:  {cm[0][0]} (correctly identified irrelevant)\")\n",
    "print(f\"False Positives: {cm[0][1]} (incorrectly marked relevant)\")\n",
    "print(f\"False Negatives: {cm[1][0]} (missed relevant papers)\")\n",
    "print(f\"True Positives:  {cm[1][1]} (correctly identified relevant)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50be2fb",
   "metadata": {},
   "source": [
    "- saving the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4858b6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = './final_biobert_classifier'\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomed_ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
