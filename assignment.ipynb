{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9b2506d",
   "metadata": {},
   "source": [
    "# BioMed: Information Retrieval - BioMedical Information Retrieval System\n",
    "\n",
    "---\n",
    "\n",
    "**Group:**\n",
    "- Reyes Castro, Didier Yamil (didier.reyes.castro@alumnos.upm.es)\n",
    "- Rodriguez Fernández, Cristina ()\n",
    "\n",
    "**Course:** BioMedical Informatics - 2025/26\n",
    "\n",
    "**Institution:** Polytechnic University of Madrid (UPM)\n",
    "\n",
    "**Date:** November 2026\n",
    "\n",
    "---\n",
    "\n",
    "## Goal\n",
    "\n",
    "To develop an Information Retrieval system — specifically, a **binary text classifier** — to identify scientific articles in the PubMed database that are related to a given set of abstracts within a defined research topic. In this case, the focus is on a collection of 1,308 manuscripts containing information on the polyphenol composition of various foods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce86cc1",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e05604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas requests transformers pytorch datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa50003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6641b8",
   "metadata": {},
   "source": [
    "## **Task 1:** \n",
    "\n",
    "Retrieve from PubMed the abstracts associated with each publication in publications.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413867ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('publications.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7c3c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n",
    "ESEARCH_URL = BASE_URL + \"esearch.fcgi\"\n",
    "FETCH_URL = BASE_URL + \"efetch.fcgi\"\n",
    "\n",
    "# Step 1: Search for the PMID of the article by title\n",
    "def search_pmid_by_title(title, api_key=None):\n",
    "    params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"term\": f\"{title}[Title]\",\n",
    "        \"retmode\": \"json\",\n",
    "        \"api_key\": api_key\n",
    "    }\n",
    "\n",
    "    try:\n",
    "\n",
    "        response = requests.get(ESEARCH_URL, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        if 'esearchresult' in data and data['esearchresult']['count'] != '0':\n",
    "            return data['esearchresult']['idlist'][0]\n",
    "        else:\n",
    "            print(f\"Found {data['esearchresult']['count']} PMIDs for title: {title}. Skipping...\")\n",
    "            return None\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error during request for title '{title}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Step 2: Fetch article abstract by PMID\n",
    "def fetch_abstract_by_pmid(pmid, api_key=None):\n",
    "    params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"id\": pmid,\n",
    "        \"retmode\": \"text\",\n",
    "        \"rettype\": \"abstract\",\n",
    "        \"api_key\": api_key\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(FETCH_URL, params=params)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching abstract for PMID '{pmid}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Process each article in the dataset\n",
    "relevant_abstracts = []\n",
    "for i, article in dataset.iterrows():\n",
    "\n",
    "    article_info = {\n",
    "        'id': article['id'],\n",
    "        'pmid': None,\n",
    "        'title': article['title'],\n",
    "        'abstract': None\n",
    "    }\n",
    "\n",
    "    title = article['title']\n",
    "    pmid = search_pmid_by_title(title)\n",
    "    \n",
    "    if pmid:\n",
    "        article_info['pmid'] = pmid\n",
    "        abstract = fetch_abstract_by_pmid(pmid)\n",
    "        article_info['abstract'] = abstract\n",
    "\n",
    "    relevant_abstracts.append(article_info)\n",
    "\n",
    "    # CHANGE ME TO 0.1 IF YOU HAVE AN API KEY\n",
    "    print(\"Sleeping for 1...\")\n",
    "    time.sleep(1)  # Delaying 1s to respect NCBI rate limits (3 requests per second)\n",
    "\n",
    "# Add relevant_abstracts to a new dataset\n",
    "relevant_df = pd.DataFrame(relevant_abstracts)\n",
    "\n",
    "# Save the updated dataset\n",
    "relevant_df.to_csv('publications_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4944556",
   "metadata": {},
   "source": [
    "There are a lot of PMIDs whose abstract is not available :( ... Ask professor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b780b",
   "metadata": {},
   "source": [
    "## **Task 2:**\n",
    "\n",
    "Use the EUtilities tool to search for articles whose content is not relevant to this task. Size of the dataset should be the same of relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c625aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_irrelevant_articles(term, count, api_key=None):\n",
    "    \n",
    "    print(f\"Fetching {count} irrelevant articles...\")\n",
    "\n",
    "    params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"term\": term,\n",
    "        \"retmode\": \"json\",\n",
    "        \"retmax\": count,\n",
    "        \"api_key\": api_key\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(ESEARCH_URL, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        if 'esearchresult' in data and data['esearchresult']['count'] != '0':\n",
    "            return data['esearchresult']['idlist']\n",
    "        else:\n",
    "            print(f\"Found {data['esearchresult']['count']} irrelevant articles.\")\n",
    "            return []\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error during request for irrelevant articles: {e}\")\n",
    "        return []\n",
    "\n",
    "irrelevant_pmids_list = search_irrelevant_articles(\"cancer[Title]\", len(dataset))\n",
    "\n",
    "irrelevant_abstracts = []\n",
    "for pmid in irrelevant_pmids_list:\n",
    "\n",
    "    article_info = {\n",
    "        'pmid': pmid,\n",
    "        'abstract': None\n",
    "    }\n",
    "\n",
    "    article_info['abstract'] = fetch_abstract_by_pmid(pmid)\n",
    "    irrelevant_abstracts.append(article_info)\n",
    "\n",
    "    # CHANGE ME TO 0.1 IF YOU HAVE AN API KEY\n",
    "    print(\"Sleeping for 1...\")\n",
    "    time.sleep(1)  # Delaying 1s to respect NCBI rate limits (3 requests per second)\n",
    "\n",
    "# Save irrelevant abstracts to a new dataset\n",
    "irrelevant_df = pd.DataFrame(irrelevant_abstracts)\n",
    "irrelevant_df.to_csv('irrelevant_publications.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6d352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: CHECK THIS OUT!! SEEMS NOT TO BE WORKING\n",
    "# There are strange abstract in the irrelevant dataset like \"1.\", erasing them and researching...\n",
    "# irrelevant_dataset_cleaned = irrelevant_dataset[~irrelevant_dataset['abstract'].str.match('1.')]\n",
    "#\n",
    "# irrelevant_dataset_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a372c85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching other irrelevant abstracts\n",
    "# new_irrelevant_pmids_list = search_irrelevant_articles(\"pneumonia[Title]\", len(dataset) - len(irrelevant_dataset), \"8e029cc2ba291ed9ee30e494f27c18017408\")\n",
    "# new_irrelevant_abstracts = []\n",
    "# for pmid in new_irrelevant_pmids_list:\n",
    "#     abstract = fetch_abstract_by_pmid(pmid, \"8e029cc2ba291ed9ee30e494f27c18017408\")\n",
    "#     new_irrelevant_abstracts.append(abstract)\n",
    "#     print(\"Sleeping for 0.1...\")\n",
    "#     time.sleep(0.1)  # Delaying 0.1s to respect NCBI rate limits (10 requests per second)\n",
    "# \n",
    "# # Adding abstracts to the irrelevant dataset\n",
    "# new_irrelevant_dataset = pd.DataFrame({'pmid': new_irrelevant_pmids_list, 'abstract': new_irrelevant_abstracts})\n",
    "# irrelevant_dataset = pd.concat([irrelevant_dataset, new_irrelevant_dataset], ignore_index=True)\n",
    "# irrelevant_dataset.to_csv('irrelevant_publications_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5092b356",
   "metadata": {},
   "source": [
    "## **Task 4:**\n",
    "\n",
    "Implement the chosen retrieval system using the programming language of their choice. If the information retrieval system is based on machine learning techniques, the student must split the existing datasets (relevant and non-relevant documents) into three distinct groups (training, validation, and testing) to carry out the model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec80f4b4",
   "metadata": {},
   "source": [
    "**CHOSEN RETRIEVAL SYSTEM:** BioBERT-based Binary Text Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8708cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding target variable 'relevance' \n",
    "relevant_df['relevance'] = 1\n",
    "irrelevant_df['relevance'] = 0\n",
    "\n",
    "# Combining relevant and irrelevant datasets and maintaining only pmid and abstract columns\n",
    "features = ['pmid', 'abstract', 'relevance']\n",
    "combined_df = pd.concat([relevant_df[features], irrelevant_df[features]], ignore_index=True)\n",
    "\n",
    "# Remove any rows where the abstract is missing (e.g., API fetch failed)\n",
    "combined_df.dropna(subset=['abstract'], inplace=True)\n",
    "combined_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Saving\n",
    "combined_df.to_csv('combined_publications.csv', index=False)\n",
    "\n",
    "print(\"Class distribution:\")\n",
    "print(combined_df['relevance'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec449e83",
   "metadata": {},
   "source": [
    "Following Fine-tuning of BioBERT for text classification tasks: https://huggingface.co/docs/transformers/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf38265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Pandas DF to HuggingFace Dataset\n",
    "combined_hf = Dataset.from_pandas(combined_df)\n",
    "\n",
    "# Tokenizing abstracts using BioBERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples['abstract'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = combined_hf.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f14a96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming targe column to 'labels' as expected by HuggingFace\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"relevance\", \"labels\")\n",
    "\n",
    "# Dividing dataset into training, evaluation and testing sets\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.2, stratify_by_column=\"labels\")\n",
    "test_validation_split = train_test_split['test'].train_test_split(test_size=0.5, stratify_by_column=\"labels\")\n",
    "\n",
    "# Final datasets\n",
    "final_datasets = {\n",
    "    'train': train_test_split['train'],\n",
    "    'validation': test_validation_split['train'],\n",
    "    'test': test_validation_split['test']\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomed_ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
